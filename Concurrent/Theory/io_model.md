# <center>IO Model</center>



<br></br>

* IO有内存IO、网络IO和磁盘IO三种，这里说的IO指的是后两者。
* 阻塞和非阻塞，是函数实现方式，即在数据就绪之前是立刻返回还是等待，即发起IO请求是否会被阻塞。
* 以文件IO为例，IO读过程是文件数据从磁盘→内核缓冲区→用户内存的过程。同步与异步区别在于数据从内核缓冲区→用户内存这个过程需不需要用户进程等待，即实际的IO读写是否阻塞请求进程。

<br></br>



## 同步阻塞
----
去餐馆吃饭，点盖浇饭然后原地等饭做好，自己端到餐桌就餐。这就是同步阻塞。当厨师给你做饭的时候，你需要一直在那里等着。

网络编程中，读取客户端的数据需要调用recvfrom。在默认情况下，这个调用会一直阻塞直到数据接收完毕，就是一个同步阻塞的IO方式。

<p align="center">
  <img src="./Images/io1.png" width=500 />
</p>

<br></br>



## 同步非阻塞
----
每次点完后，回桌子坐着，然后估计差不多了，就问老板饭好了没，如果好了就去端，没好就等一会再问，依次循环。这就是同步非阻塞。

这种方式在编程中对socket设置`O_NONBLOCK`即可。但此方式仅对网络IO有效，对磁盘IO没有作用。因为本地文件IO没被认为是阻塞，网络IO阻塞因为网路IO有无限阻塞可能，而本地文件除非被锁住，否则不可能无限阻塞。而且，磁盘IO时要么数据在内核缓冲区中直接可以返回，要么需调用物理设备读取，这时进程其他工作都需要等待。因此，后续的IO复用和信号驱动IO对文件IO也没有意义。

> Nginx和Node对本地文件IO是用线程方式模拟非阻塞。

<p align="center">
  <img src="./Images/io2.png" width=500 />
</p>

<br>


### IO复用
点一份饭然后循环去问好没有点得不偿失，不如等到准备好，但当点了好几样饭菜时，每次都去问一下所有饭菜状态(未做好/已做好)肯定比每次阻塞在那里等着好。当然，问时需要阻塞，一直到有准备好的饭菜或者你等的不耐烦(超时)。这就引出了IO复用，也叫多路IO就绪通知。这是一种进程预先告知内核的能力，让内核发现进程指定的一个或多个IO条件就绪了，就通知进程。使得一个进程能在一连串的事件上等待。

<p align="center">
  <img src="./Images/io3.png" width=500 />
</p>

<br>

IO复用的实现方式主要有select、poll和epoll。

select和poll的原理基本相同：
1. 注册待侦听的fd(这里的fd创建时最好使用非阻塞)
2. 每次调用都去检查这些fd的状态，当有一个或者多个fd就绪的时候返回
3. 返回结果中包括已就绪和未就绪的fd

相比select，poll解决了单个进程能够打开的文件描述符数量限制，select受限于`FD_SIZE`限制，如果修改则需修改宏重新编译内核；而poll通过一个pollfd数组向内核传递需要关注的事件，避开了文件描述符数量限制。

此外，select和poll共有的缺点是包含大量fd的数组被整体复制于用户态和内核态地址空间之间，开销随着fd数量增多而线性增大。

select和poll类似于上面说的就餐方式。但当每次都去询问时，老板会把所有点的饭菜都轮询一遍再告诉你情况，当大量饭菜很长时间都不能准备好的情况下很低效。于是，老板不耐烦，让厨师每做好一个菜就通知他。这样每次再去问时，他直接把已准备好的菜告诉你。**这就是事件驱动IO就绪通知的方式-epoll。**

epoll的出现，解决了select、poll的缺点：
1. 基于事件驱动的方式，避免了每次都要把所有fd都扫描一遍。
2. `epoll_wait`只返回就绪的fd。
3. epoll使用nmap内存映射技术避免了内存复制的开销。
4. epoll的fd数量上限是操作系统的最大文件句柄数目,这个数目一般和内存有关，通常远大于1024。

> 目前，epoll是Linux2.6下最高效的IO复用方式，也是Nginx、Node的IO实现方式。而在freeBSD下，kqueue是另一种类似于epoll的IO复用方式。

对于IO复用还有一个水平触发和边缘触发的概念：
* 水平触发 当就绪的fd未被用户进程处理后，下一次查询依旧会返回，这是select和poll的触发方式。epoll默认使用水平触发。
* 边缘触发 无论就绪的fd是否被处理，下一次不再返回。理论上性能高但实现复杂，且任何意外的丢失都会造成请求处理错误。

<br>


### 信号驱动
上文就餐方式还是需要每次去问饭菜状况。于是，你不耐烦了，跟老板说哪个饭菜好了就通知一声。然后就干自己的事情。更甚者，可把手机号留给老板，等饭菜好了直接发短信。这就类似信号驱动的IO模型。

<p align="center">
  <img src="./Images/io4.png" width=500 />
</p>

<br>

流程如下：
1. 开启套接字信号驱动IO功能
2. 系统调用`sigaction`执行信号处理函数（非阻塞，立刻返回）
3. 数据就绪，生成sigio信号，通过信号回调通知应用来读取数据。

此种io方式存在一个问题：Linux中信号队列是有限制的，如果超过这个数字问题就无法读取数据。

<br></br>



## 异步非阻塞（事件驱动）
----
之前就餐方式，到最后总需要自己去把饭菜端到餐桌。这下不耐烦了，于是告诉老板，饭好了直接端到面前。这就是异步非阻塞IO。

<p align="center">
  <img src="./Images/io5.png" width=500 />
</p>

<br>

对比信号驱动IO，异步IO的主要区别在于：信号驱动由内核告诉何时开始一个IO操作(数据在内核缓冲区中)，而异步IO则由内核通知IO操作何时完成(数据已在用户空间)。

<br></br>



## 网络编程模型
----
上文讲述了UNIX五种IO模型。Java具有以下几种网络编程模型：
1. BIO
2. NIO
3. AIO
4. BIO

<br>


### BIO
是一个典型的网络编程模型，是通常实现一个服务端程序的过程，步骤如下：
1. 主线程accept请求阻塞
2. 请求到达，创建新的线程来处理这个套接字，完成对客户端的响应。
3. 主线程继续accept下一个请求

当客户端连接增多时，服务端创建的线程也会暴涨，系统性能急剧下降。因此，在此模型的基础上，类似于Tomcat的bio connector，采用的是线程池来避免对于每一个客户端都创建一个线程。

<br>


### NIO
JDK1.4引入NIO类库，指Non-blcok IO，主要使用selector多路复用器实现。selector在Linux上通过epoll实现。NIO的实现流程，类似于select：
1. 创建ServerSocketChannel监听客户端连接并绑定监听端口，设置为非阻塞模式。
2. 创建Reactor线程，创建多路复用器并启动线程。
3. 将ServerSocketChannel注册到Reactor线程的selector上，监听accept事件。
4. selector在线程run方法中无线循环轮询准备就绪的Key。
5. selector监听到新的客户端接入，处理新的请求，完成TCP三次握手，建立物理连接。
6. 将新客户端连接注册到selector上，监听读操作。
7. 客户端发送的数据就绪则读取客户端请求，进行处理。

<br>


### AIO
JDK1.7引入NIO2.0，提供异步文件通道和异步套接字通道实现。其底层在windows上通过IOCP，在Linux上通过epoll实现。其步骤为：
1. 创建AsynchronousServerSocketChannel，绑定监听端口
2. 调用AsynchronousServerSocketChannel的accpet方法，传入自己实现的CompletionHandler。包括上一步，都是非阻塞的
3. 连接传入，回调CompletionHandler的completed方法，在里面，调用AsynchronousSocketChannel的read方法，传入负责处理数据的CompletionHandler。
4. 数据就绪，触发负责处理数据的CompletionHandler的completed方法。
5. 写入操作类似，也需要传入CompletionHandler。

<br>


### 对比

|                    | 同步阻塞IO |    伪异步IO |         NIO |   AIO |
| :----------------- | :-------: | :--------: | :--------: | :---: |
| 客户端数目（IO线程）  | 1 : 1     |  m : n     |      m : 1 | m : 0 |
| IO模型              | 同步阻塞IO | 同步非阻塞IO | 同步非阻塞IO | 异步非阻塞IO |
| 吞吐量              | 低        | 中          |    高      |  高   |
| 编程复杂度          |	  简单    |  简单       |	非常复杂   | 	复杂 |	
				
